{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Preprocess Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokenized sentences in train set is: 808180\n",
      "\n",
      "sample of the train set: \n",
      "\n",
      "['he', 'had', 'a', 'large', 'impact', 'on', 'a', 'lot', 'of', 'people', 'he', 'said']\n",
      "['once', 'full', 'of', 'food', 'and', 'vodka', 'and', 'thanking', 'borat', 'would', 'finally', 'start', 'to', 'relax', 'so', 'much', 'that', 'hed', 'want', 'to', 'show', 'off', 'every', 'one', 'of', 'his', 'thankingday', 'learnings']\n",
      "['already', 'advertising', 'trips', 'to', 'upcoming', 'festivals', 'at', 'merriweather', 'rock', 'bus', 'intends', 'to', 'stay', 'in', 'business', 'akram', 'said']\n",
      "['i', 'just', 'want', 'to', 'forget', 'about', 'everything', 'said', 'sayaka', 'shimura', '25', 'carrying', 'a', 'mickey', 'mouseemblazoned', 'tote', 'bag', 'with', 'various', 'disney', 'charms', 'dangling', 'off', 'her', 'cellphone', 'i', 'go', 'to', 'disneyland', 'two', 'to', 'three', 'times', 'a', 'month', 'and', 'i', 'took', 'the', 'day', 'off', 'work', 'today', 'so', 'i', 'could', 'go', 'i', 'am', 'so', 'excited']\n",
      "['union', 'county', 'prosecutor', 'theodore', 'romankow', 'said', 'hes', 'worried', 'municipalities', 'will', 'reduce', 'police', 'ranks', 'because', 'of', 'tight', 'budgets']\n",
      "\n",
      "The number of tokenized sentences in test set is: 202046\n",
      "\n",
      "sample of the test set: \n",
      "\n",
      "['a', 'i', 'really', 'dont', 'know', 'at', 'this', 'point', 'i', 'mean', 'i', 'might', 'continue', 'in', 'acting', 'if', 'i', 'get', 'good', 'opportunities', 'that', 'i', 'find', 'interesting', 'that', 'have', 'a', 'good', 'director', 'and', 'a', 'good', 'script', 'and', 'great', 'actors', 'who', 'id', 'like', 'to', 'work', 'with', 'so', 'far', 'this', 'has', 'been', 'a', 'really', 'great', 'experience', 'with', 'this', 'one', 'film', 'everyone', 'has', 'been', 'so', 'nice', 'and', 'so', 'wonderful', 'to', 'me', 'and', 'thats', 'been', 'really', 'great', 'but', 'on', 'the', 'flip', 'side', 'i', 'get', 'that', 'it', 'isnt', 'always', 'going', 'to', 'be', 'like', 'that', 'there', 'are', 'certainly', 'other', 'things', 'id', 'like', 'to', 'do', 'with', 'my', 'life', 'as', 'well']\n",
      "['and', 'then', 'we', 'come', 'to', 'tapas', 'try', 'not', 'to', 'leave', 'valencia', 'without', 'having', 'a', 'tapas', 'dinner', 'at', 'casa', 'montaa', 'emilianobodegacom', 'an', 'old', 'bodega', 'located', 'in', 'el', 'cabaal', 'a', 'fishing', 'village', 'near', 'malvarrosa', 'beach', 'you', 'also', 'must', 'try', 'the', 'trendy', 'tapas', 'at', 'the', 'new', 'hip', 'restaurant', 'vuelve', 'carolina', 'vuelvecarolinacom', 'and', 'dont', 'miss', 'the', 'oldfashioned', 'pintxos', 'basque', 'word', 'for', 'tapas', 'on', 'the', 'first', 'level', 'of', 'sagardi', 'sagardicom', 'language', 'isnt', 'necessary', 'at', 'this', 'fun', 'bar', 'as', 'you', 'help', 'yourself', 'to', 'the', 'snacks', 'each', 'stuck', 'with', 'a', 'toothpick', 'and', 'then', 'pay', 'as', 'you', 'leave', 'the', 'bill', 'calculated', 'by', 'the', 'number', 'of', 'toothpicks', 'left', 'on', 'the', 'plate']\n",
      "['history', 'colorado', 'opens', 'its', 'grand', 'new', 'home', 'at', '1200', 'broadway', 'on', 'april', '28', 'this', 'stateoftheart', 'museum', 'mixes', 'new', 'high', 'tech', 'interactive', 'exhibits', 'with', 'treasures', 'collected', 'ever', 'since', 'colorados', 'puppy', 'days']\n",
      "['q', 'you', 'included', 'four', 'cleveland', 'neighborhoods', 'why']\n",
      "['anglemier', 'said', 'the', 'policeman', 'involved', 'in', 'the', 'shooting', 'was', 'officer', 'oscar', 'zambrano', 'a', 'sixyear', 'veteran', 'of', 'the', 'salem', 'force']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_and_preprocess(file_path):\n",
    "    tokenized_sentences = []\n",
    "    with open(file_path, 'r', encoding='UTF-8') as file: #opening the file\n",
    "        for line in file: # iterating over the file line by line\n",
    "            line = line.strip().lower() # removing space at the start and en Â¡d of the file and also converting all letters to lowercase\n",
    "            line = re.sub(r'[^a-zA-Z0-9\\s]', '', line) # removing punctuation\n",
    "            tokens =line.split() #tokenizing by space\n",
    "            if tokens:\n",
    "                tokenized_sentences.append(tokens)\n",
    "    return tokenized_sentences\n",
    "\n",
    "dataset_path = \"/Users/mac/Desktop/Grad/NLP/NLP/archive-2/final/en_US/en_US.news.txt\"\n",
    "tokenized_data = load_and_preprocess(dataset_path)\n",
    "\n",
    "train_set, test_set = train_test_split(tokenized_data, test_size = 0.2, random_state = 42)\n",
    "print(f\"The number of tokenized sentences in train set is: {len(train_set)}\")\n",
    "print(\"\\nsample of the train set: \\n\")\n",
    "for i in range(5):\n",
    "    print(train_set[i])\n",
    "print(f\"\\nThe number of tokenized sentences in test set is: {len(test_set)}\")\n",
    "print(\"\\nsample of the test set: \\n\")\n",
    "for j in range(5):\n",
    "    print(test_set[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 samples from training data:\n",
      "['he', 'had', 'a', 'large', 'impact', 'on', 'a', 'lot', 'of', 'people', 'he', 'said']\n",
      "['once', 'full', 'of', 'food', 'and', 'vodka', 'and', 'thanking', 'borat', 'would', 'finally', 'start', 'to', 'relax', 'so', 'much', 'that', 'hed', 'want', 'to', 'show', 'off', 'every', 'one', 'of', 'his', 'thankingday', 'learnings']\n",
      "['already', 'advertising', 'trips', 'to', 'upcoming', 'festivals', 'at', 'merriweather', 'rock', 'bus', 'intends', 'to', 'stay', 'in', 'business', 'akram', 'said']\n",
      "['i', 'just', 'want', 'to', 'forget', 'about', 'everything', 'said', 'sayaka', 'shimura', '25', 'carrying', 'a', 'mickey', 'mouseemblazoned', 'tote', 'bag', 'with', 'various', 'disney', 'charms', 'dangling', 'off', 'her', 'cellphone', 'i', 'go', 'to', 'disneyland', 'two', 'to', 'three', 'times', 'a', 'month', 'and', 'i', 'took', 'the', 'day', 'off', 'work', 'today', 'so', 'i', 'could', 'go', 'i', 'am', 'so', 'excited']\n",
      "['union', 'county', 'prosecutor', 'theodore', 'romankow', 'said', 'hes', 'worried', 'municipalities', 'will', 'reduce', 'police', 'ranks', 'because', 'of', 'tight', 'budgets']\n",
      "\n",
      "First 5 samples from test data:\n",
      "['a', 'i', 'really', 'dont', 'know', 'at', 'this', 'point', 'i', 'mean', 'i', 'might', 'continue', 'in', 'acting', 'if', 'i', 'get', 'good', 'opportunities', 'that', 'i', 'find', 'interesting', 'that', 'have', 'a', 'good', 'director', 'and', 'a', 'good', 'script', 'and', 'great', 'actors', 'who', 'id', 'like', 'to', 'work', 'with', 'so', 'far', 'this', 'has', 'been', 'a', 'really', 'great', 'experience', 'with', 'this', 'one', 'film', 'everyone', 'has', 'been', 'so', 'nice', 'and', 'so', 'wonderful', 'to', 'me', 'and', 'thats', 'been', 'really', 'great', 'but', 'on', 'the', 'flip', 'side', 'i', 'get', 'that', 'it', 'isnt', 'always', 'going', 'to', 'be', 'like', 'that', 'there', 'are', 'certainly', 'other', 'things', 'id', 'like', 'to', 'do', 'with', 'my', 'life', 'as', 'well']\n",
      "['and', 'then', 'we', 'come', 'to', 'tapas', 'try', 'not', 'to', 'leave', 'valencia', 'without', 'having', 'a', 'tapas', 'dinner', 'at', 'casa', 'montaa', 'emilianobodegacom', 'an', 'old', 'bodega', 'located', 'in', 'el', 'cabaal', 'a', 'fishing', 'village', 'near', 'malvarrosa', 'beach', 'you', 'also', 'must', 'try', 'the', 'trendy', 'tapas', 'at', 'the', 'new', 'hip', 'restaurant', 'vuelve', 'carolina', 'vuelvecarolinacom', 'and', 'dont', 'miss', 'the', 'oldfashioned', 'pintxos', 'basque', 'word', 'for', 'tapas', 'on', 'the', 'first', 'level', 'of', 'sagardi', 'sagardicom', 'language', 'isnt', 'necessary', 'at', 'this', 'fun', 'bar', 'as', 'you', 'help', 'yourself', 'to', 'the', 'snacks', 'each', 'stuck', 'with', 'a', 'toothpick', 'and', 'then', 'pay', 'as', 'you', 'leave', 'the', 'bill', 'calculated', 'by', 'the', 'number', 'of', 'toothpicks', 'left', 'on', 'the', 'plate']\n",
      "['history', 'colorado', 'opens', 'its', 'grand', 'new', 'home', 'at', '1200', 'broadway', 'on', 'april', '28', 'this', 'stateoftheart', 'museum', 'mixes', 'new', 'high', 'tech', 'interactive', 'exhibits', 'with', 'treasures', 'collected', 'ever', 'since', 'colorados', 'puppy', 'days']\n",
      "['q', 'you', 'included', 'four', 'cleveland', 'neighborhoods', 'why']\n",
      "['anglemier', 'said', 'the', 'policeman', 'involved', 'in', 'the', 'shooting', 'was', 'officer', 'oscar', 'zambrano', 'a', 'sixyear', 'veteran', 'of', 'the', 'salem', 'force']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_and_preprocess_text(file_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses a given text dataset.\n",
    "    - Tokenizes each sentence into words.\n",
    "    - Returns a list of tokenized sentences.\n",
    "    \"\"\"\n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip().lower()  # Convert to lowercase\n",
    "            line = re.sub(r'[^a-zA-Z0-9\\s]', '', line)  # Remove punctuation\n",
    "            tokens = line.split()  # Tokenize\n",
    "            if tokens:  # Skip empty lines\n",
    "                tokenized_sentences.append(tokens)\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "# Path to the dataset (Make sure to place the correct path)\n",
    "#dataset_path = \"en_US.twitter.txt\"  # Update this with the actual path if different\n",
    "\n",
    "# Load and preprocess the text dataset\n",
    "tokenized_data = load_and_preprocess_text(dataset_path)\n",
    "\n",
    "# Split into training and test sets\n",
    "train_data, test_data = train_test_split(tokenized_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print first 5 samples from each dataset\n",
    "print(\"First 5 samples from training data:\")\n",
    "for i in range(5):\n",
    "    print(train_data[i])\n",
    "\n",
    "print(\"\\nFirst 5 samples from test data:\")\n",
    "for i in range(5):\n",
    "    print(test_data[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
